## Package managers: Installing stuff

Research pipelines in population genetics and evolutionary biology juggle many CLI toolsâ€”**`samtools`, `bcftools`, `RAxMLâ€‘NG`, `IQâ€‘TREE`, `PLINK`, `FastQC`, `BLAST+`,** and more. Manually compiling each one wastes time and risks version drift that can invalidate downstream analyses. A package manager:

* **Pins exact versions** so workflows run identically on every machine
* **Resolves dependencies automatically** (e.g., HTSlibâ€¯â†”â€¯samtoolsâ€¯â†”â€¯bcftools)
* **Delivers security patches** for system libraries without manual rebuilding
* **Provides oneâ€‘line cleanup** to reclaim space after large FASTQ/VCF crunches

---

## Two ecosystems at a glance (no extra repositories required)

| Aspect                              | `apt` (Ubuntu/Debian)                                                              | `brew` (macOSâ€¯+â€¯Linux)                                                                                     |
| ----------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| Bioâ€‘relevant packages in core repos | `samtools`, `bcftools`, `mafft`, `raxmlâ€‘ng`, `iqtree`, `blast+`, `fastqc`, `plink` | Same in **Homebrew core** (`samtools`, `bcftools`, `raxmlâ€‘ng`, `iqtree`, `plink`, `fastqc`, `blast`, etc.) |
| Install root                        | Systemâ€‘wide `/usr/bin`, `/usr/lib` (needs `sudo`)                                  | Userâ€‘level Cellar, symlinked to `/usr/local/bin` or `/opt/homebrew/bin` (no `sudo`)                        |
| Upgrade strategy                    | `sudo apt upgrade` / `sudo apt full-upgrade`                                       | `brew upgrade`                                                                                             |
| Cleanup & health check              | `sudo apt autoremove` / `sudo apt clean`                                           | `brew cleanup` & `brew doctor`                                                                             |

---

## Bestâ€‘practice workflowÂ (for standard repos only)

1. **Refresh metadata & upgrade**

   ```bash
   # Ubuntu
   sudo apt update && sudo apt upgrade -y

   # macOS / Linuxbrew
   brew update && brew upgrade
   ```

2. **Search & inspect before installing**

   ```bash
   apt search iqtree | head
   brew info iqtree
   ```

3. **Dryâ€‘run heavy installs** (good on shared HPC nodes)

   ```bash
   sudo apt install --simulate bcftools
   brew install --dry-run iqtree
   ```

4. **Install the toolchain**

   ```bash
   # Population genetics & phylogenomics
   sudo apt install samtools bcftools plink raxml-ng iqtree fastqc blast+

   # Same on macOS
   brew install samtools bcftools plink raxml-ng iqtree fastqc blast
   ```

5. **Pin versions for published pipelines**

   ```bash
   echo "iqtree hold" | sudo dpkg --set-selections    # Ubuntu
   brew pin iqtree                                    # macOS
   ```

6. **Audit & prune monthly**

   ```bash
   sudo apt autoremove --purge
   brew cleanup
   brew outdated
   ```

---

<!-- ## Examples

<details>
<summary>ðŸ§¬Â Call SNPs from BAM to VCF on Ubuntu</summary>

```bash
sudo apt install samtools bcftools
samtools mpileup -Ou -f ref.fa sample.bam | \
  bcftools call -mv -Oz -o sample.vcf.gz
```

Matching **samtoolsâ€¯+â€¯bcftools** builds linked against the same HTSlib avoid ABI mismatches.

</details>

<details>
<summary>ðŸŒ³Â Maximumâ€‘likelihood phylogeny with IQâ€‘TREE on macOS</summary>

```bash
brew install iqtree
iqtree -s alignment.phy -m GTR+G -B 1000 -nt AUTO
open alignment.phy.treefile  # View in FigTree if installed via brew install --cask figtree
```

Homebrew compiles IQâ€‘TREE with native optimizations, accelerating bootstrap analyses.

</details>

<details>
<summary>ðŸ“ŠÂ Prepare genotype matrices for STRUCTURE / ADMIXTURE</summary>

```bash
sudo apt install plink
plink --vcf cohort.vcf --make-bed --out cohort
```

Using the package manager lets collaborators reproduce the exact `plink` build with a single command.

</details>

--- -->

## Troubleshooting quickâ€‘ref

| Issue                                       | `apt` fix                                           | `brew` fix                                                                 |
| ------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |
| HTSlib version mismatch                     | `sudo apt install --only-upgrade htslib`            | `brew upgrade htslib samtools bcftools`                                    |
| Missing libraries when compiling R packages | `sudo apt install libcurl4-openssl-dev libxml2-dev` | `brew install libxml2 curl openssl@3 && brew link --force libxml2`         |
| Permission denied                           | Always prefix with `sudo` or check `/etc/sudoers`   | Ensure you own Homebrew dirs: `sudo chown -R $(whoami) $(brew --prefix)/*` |
| Conflicting files                           | `sudo apt --fix-broken install`                     | `brew link --overwrite <pkg>` or `brew unlink <pkg>`                       |

---

### Takeâ€‘home points for lifeâ€‘science researchers

1. **Reproducibility first** â€” lock tool versions before paper submission.
2. **Standard repos suffice** â€” many bioinformatics staples (including **IQâ€‘TREE**) are in default `apt` and Homebrew core.
3. **Simulate big installs** on clusters to dodge quota surprises.
4. **Document your environment** (`apt list --installed > packages.txt`, `brew bundle dump`) and commit alongside your code.

With these habits, your phylogenetic trees, GWAS scans, and floristic datasets will stay **repeatable and transparent**â€”boosting confidence in every published clade and alleleâ€‘frequency plot. Happy analysing!

## Pipes

Below are five selfâ€‘contained miniâ€‘pipelines that chain **only standard GNU/Linux utilities** (`find`, `xargs`, `grep`, `awk`, `sort`, `uniq`, `cut`, `tr`, `wc`, `tee`,â€¯etc.). Theyâ€™re meant as â€œlego bricksâ€ you can demo live to show how small commands combine into surprisingly rich workflows.

---

### 1.â€¯Find the ten most abundant IP addresses in a webâ€‘server log

```bash
cat access.log |        # stream the whole log
awk '{print $1}' |      # 1st column is the client IP
sort |                  # bring identical IPs together
uniq -c |               # collapse & count duplicates
sort -nr |              # numeric reverse sort by hits
head -10                # show top 10
```

**Why itâ€™s neat:** six short commands replace an entire logâ€‘analysis script.

---

### 2.â€¯Count reads across **every** gzipped FASTQ in the project directory

```bash
find . -name '*.fastq.gz' -print0 |   # locate all FASTQ files
xargs -0 zcat |                       # concatenate & decompress in one pass
wc -l |                               # total number of lines
awk '{print $1/4 " reads"}'           # 4 lines per read â†’ read count
```

No temporary files, works on terabyteâ€‘scale data.

---

### 3.â€¯Extract coding sequences from a GFF3, then tally the ten longest genes

```bash
grep -P '\tCDS\t' annotation.gff3 |      # keep only CDS features
cut -f1,4,5,9 |                          # grab seqâ€‘id, start, end, attributes
awk -F'\t' '{len=$3-$2+1; print len"\t"$4}' |  # compute length, keep attributes
sort -nr | head -10                       # top 10 by length
```

Shows off tabâ€‘aware filtering plus onâ€‘theâ€‘fly arithmetic in a single stream.

---

### 4.â€¯Compute the GC content of all contigs in a FASTA file

```bash
awk '/^>/ {if(len){gc/len*100; printf "\t%.2f%%\n", gc/len*100}; \
           len=0; gc=0; printf "%s", $0; next} \
     {len+=length($0); gc+=gsub(/[GCgc]/,"") } \
     END {gc/len*100; printf "\t%.2f%%\n", gc/len*100}' \
     genome.fa | column -t
```

A single `awk` scriptâ€”piped from nothing but the fileâ€”prints each header with its GC%.

---

### 5.â€¯Live progress bar while uncompressing, sorting, and deduplicating a giant VCF

```bash
pv big.vcf.gz |          # progressâ€‘view the bytes as they stream
zcat |                   # decompress
grep -v '^##' |          # drop metaâ€‘header lines, keep data & column header
sort --buffer-size=1G |  # external merge sort without temp scripting
uniq |                   # remove duplicate variant lines
tee cleaned.vcf | wc -l  # save output *and* report final variant count
```

Demonstrates pipes branching (`tee`), inâ€‘pipe monitoring (`pv`), and memoryâ€‘safe sorting.

---

### Key teaching points to highlight during the demo

* **Pipelines are just streams:** every tool readsâ€¯stdin, writesâ€¯stdout, so composition is trivial.
* **Stateless commands scale:** utilities donâ€™t load entire files into RAM (unless told to), so they handle genomes or logs severalâ€¯GB large.
* **Swap tools freely:** replace `uniq -c` with `awk` math, or `grep` with `ripgrep`â€”the interface stays the same.
* **Debug incrementally:** add `head`, `less`, or `tee` anywhere to peek at intermediate output without rerunning everything.

Each oneâ€‘liner illustrates how small, wellâ€‘focused GNU programs become far more powerful when they â€œtalkâ€ to each other through pipes. Encourage learners to treat these commands like puzzle piecesâ€”and never be afraid to string together a few more!


### A (very) quick primer on *pipes*

On Unixâ€‘like systems every program can read **standard input** (stdin) and write **standard output** (stdout).
The **pipe operator `|`** connects these streams, so the output of one tool becomes the input of the nextâ€”no temporary files, no extra disk I/O. Because most bioinformatics data (FASTA, FASTQ, GFF, VCFÂ â€¦) is just text, you can assemble powerful adâ€‘hoc workflows by chaining tiny, wellâ€‘tested GNU utilities instead of writing bespoke scripts.

```
toolâ€‘A   |   toolâ€‘B   |   toolâ€‘C
stdin â†’ stdout â†’ stdin â†’ stdout â†’ â€¦
```

Key benefits for scientists:

* **Reproducibility** â€“ every step is visible in one line; pop it into a README or Snakemake rule.
* **Scalability** â€“ tools stream data, so terabyteâ€‘scale files stay off your RAM.
* **Flexibility** â€“ swap any component (e.g., `grep` â†’ `rg`) without rewriting everything else.

---

## Five bioinformaticsâ€‘flavoured pipelines

---

#### 1ï¸âƒ£Â How many raw reads are in *all* gzipped FASTQ files under the current project?

```bash
find . -name '*.fastq.gz' -print0 |   # locate every FASTQ
xargs -0 zcat            |            # streamâ€‘decompress
wc -l                    |            # total line count
awk '{printf "%.0f reads\n", $1/4}'   # 4 lines per read
```

---

#### 2ï¸âƒ£Â Get the ten longest coding genes from a reference GFF3

```bash
grep -P '\tCDS\t' annotation.gff3 |    # keep only CDS features
awk -F'\t' '{len=$5-$4+1; print len"\t"$9}' |
sort -nr | head -10
```

Shows arithmetic in `awk`, numeric sorting, and quick rankingâ€”no Python needed.

---

#### 3ï¸âƒ£Â Perâ€‘contig GCâ€‘content table for a multiâ€‘FASTA genome

```bash
awk '/^>/ {if(NR>1) printf("\t%.2f%%\n", gc/len*100);
           gc=len=0; printf "%s", $0; next}
     {len+=length; gc+=gsub(/[GCgc]/,"")}
     END {printf("\t%.2f%%\n", gc/len*100)}' genome.fa |
column -t
```

A single `awk` pass prints each header followed by its GC%.

---

#### 4ï¸âƒ£Â Filter a VCF to highâ€‘quality biallelic SNPs **and** count how many you keep

```bash
grep -v '^##' sample.vcf |          # drop metaâ€‘header
awk '$7=="PASS" && length($4)==1 && length($5)==1' |  # SNPâ€‘only
tee clean.vcf | wc -l
```

`tee` saves the filtered file *and* lets you pipe the same stream into `wc`.

---

#### 5ï¸âƒ£Â Realâ€‘time progress while deduplicating a huge VCF dump

```bash
pv big.vcf.gz |          # progress bar (bytes / ETA)
zcat | grep -v '^##' | sort --buffer-size=1G | uniq | \
tee unique.vcf | awk 'END{print NR" unique variants"}'
```

Demonstrates monitoring (`pv`), memoryâ€‘safe external sorting, branching with `tee`, and a final summary.

---

### Takeâ€‘home message

Think of each GNU utility as a **LEGO brick**: simple and reliable on its own, but unexpectedly powerful when snapped into a pipeline. Mastering `|` (plus redirection `>`, subshells `$( )`, and `tee`) will make every genome assembly, variant callâ€‘set, or readâ€‘counting job faster to prototypeâ€”and easier for your collaborators to reproduce.

## A deeper look at â€œpipesâ€ for bioinformaticsâ€¯workflows

When you type a command in the shell, it receives three default data streams:

| Stream     | File descriptor | Default destination         | Why bioinformaticians care                                          |
| ---------- | --------------- | --------------------------- | ------------------------------------------------------------------- |
| **stdin**  | `0`             | keyboard / previous command | Read huge FASTA/VCF files without loading them fully into RAM       |
| **stdout** | `1`             | terminal                    | Hand results to the *next* tool instead of creating temporary files |
| **stderr** | `2`             | terminal (error channel)    | Keep warnings separate from the data stream so pipelines stay clean |

The **pipe operator `|`** connects `stdout` of one program directly to `stdin` of the next:

```text
aligner â”€â”¬â”€â–º stdout â”‚
         â”‚          â”‚   sorter  â”€â”¬â”€â–º stdout â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚          â”‚   summarizer â”€â”€â–º results
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why pipes shine in lifeâ€‘science data crunching

* **Streaming, not loading** â€“ Most GNU tools process data lineâ€‘byâ€‘line, so a 100â€¯GB BAM file never sits entirely in memory.
* **Composability** â€“ Each command does one thing well (Unix philosophy). Chain them to build adâ€‘hoc â€œminiâ€‘workflowsâ€ in seconds.
* **Reproducibility** â€“ A single line in a README or Snakemake rule documents every stepâ€”no hidden GUI clicks.
* **Portability** â€“ Every modern Linux distribution ships the same coreutils, `grep`, `awk`, `sort`, `uniq`, etc.

---

## Five detailed, biologyâ€‘centric pipelines

Below each pipeline youâ€™ll find *what it does* and *why it matters*. All commands rely solely on tools that come with a fresh Ubuntu or Debian install (plus `pv`, which is in `apt` and Homebrew core).

---

### 1ï¸âƒ£Â Count raw reads in every gzipped FASTQ below the current directory

```bash
find . -name '*.fastq.gz' -print0 |     # â‘  list all FASTQ files, NULâ€‘separated
xargs -0 zcat            |              # â‘¡ decompress safely (NUL handles spaces)
wc -l                    |              # â‘¢ total number of lines
awk '{printf "%.0f reads\n", $1/4}'     # â‘£ convert lines â†’ reads (4 lines/read)
```

*Why itâ€™s useful*
You can sanityâ€‘check a sequencing run before alignment and catch truncated uploads (read count mismatch) **without** ever writing decompressed data to disk.

*Key pipe concepts*
`findÂ â€¦Â -print0` + `xargsÂ -0` forms a bulletâ€‘proof pair that survives filenames with spaces or emojis.

---

### 2ï¸âƒ£Â Rank the longest coding sequences (genes) in a GFF3 annotation

```bash
grep -P '\tCDS\t' annotation.gff3 |              # â‘  keep only CDS rows
awk -F'\t' '{len=$5-$4+1; print len"\t"$1":"$4"-"$5"\t"$9}' | \
                                             # â‘¡ compute length; print length,
                                             #    coordinates, attributes
sort -nr | head -10                            # â‘¢ numeric reverseâ€‘sort & top 10
```

*Why itâ€™s useful*
Spotting unusually long proteins helps detect possible annotation errors or giant gene families worth further study.

*Key pipe concepts*
You can inline arithmetic (`awk`) and ranking (`sort | head`) without ever opening Python/R.

---

### 3ï¸âƒ£Â Produce a perâ€‘contig GCâ€‘content table for a genome FASTA

```bash
awk '
/^>/ {                       # header line
  if (NR>1) printf "\t%.2f%%\n", gc/len*100;  # print previous contig stats
  gc=len=0;                               # reset counters
  printf "%s", $0; next                   # echo header without newline
}
{ len += length($0); gc += gsub(/[GCgc]/, "") }   # count bases & G/C
END { printf "\t%.2f%%\n", gc/len*100 }           # final contig
' genome.fa | column -t
```

*Why itâ€™s useful*
GC bias affects read mapping, assembly, and variant calling. A quick perâ€‘contig table can diagnose odd coverage patterns.

*Key pipe concepts*
A *single* `awk` script does aggregation, math, and formatted printing; `column` makes the output pretty without extra code.

---

### 4ï¸âƒ£Â Filter and count highâ€‘quality biallelic SNPs in a VCF

```bash
grep -v '^##' sample.vcf |                                 # â‘  strip metaâ€‘headers
awk '$7=="PASS" && length($4)==1 && length($5)==1' |       # â‘¡ keep PASS + SNPs
tee clean_snps.vcf |                                       # â‘¢ save stream to file
wc -l |                                                    # â‘£ count remaining rows
awk '{print $1-1 " SNPs retained"}'                        # minus the #CHROM header
```

*Why itâ€™s useful*
Demonstrates how you can **annotate** your own QC thresholds directly in the pipeline and verify the result in real time.

*Key pipe concepts*
`tee` branches the stream: one leg to disk, the other to `wc`, so no second pass is needed.

---

### 5ï¸âƒ£Â See live progress while deduplicating a massive VCF (or any large text file)

```bash
pv big.vcf.gz |                     # â‘  show MB/s, ETA
zcat |                              # â‘¡ decompress
grep -v '^##' |                     # â‘¢ drop metaâ€‘headers
sort --buffer-size=1G |             # â‘£ external merge sort, 1â€¯GB RAM cap
uniq |                              # â‘¤ collapse duplicates
tee unique.vcf |                    # â‘¥ save cleaned file
awk 'END{print NR-1" unique variants"}'  # â‘¦ final count (minus header)
```

*Why itâ€™s useful*
Genotypers sometimes emit duplicate lines. This pipeline removes them **safely** on a laptop with limited memory while showing how long it will take.

*Key pipe concepts*

| Concept           | Command               | Why it helps                                  |
| ----------------- | --------------------- | --------------------------------------------- |
| **Monitoring**    | `pv`                  | Immediate feedback for multiâ€‘minute jobs      |
| **Resource caps** | `sortÂ --buffer-size=` | Prevents swapping on lowâ€‘RAM machines         |
| **Branching**     | `tee`                 | Simultaneous save + stats, no extra disk read |

---

### General best practices for piping in bioscience

1. **Fail early**: Insert `setÂ -oÂ pipefail` in Bash scripts so an error in any segment aborts the entire pipeline.
2. **Quote and NULâ€‘terminate**: Use `findÂ -print0`/`xargsÂ -0` or `whileÂ IFS= read -r -d '' file; do â€¦; done` loops to survive funky filenames.
3. **Document as you go**: Comment inline (`# â€¦`) so future youâ€”or a peer reviewerâ€”knows *why* each filter exists.
4. **Keep streams pure**: Reserve `stderr` (`>&2`) for warnings so `stdout` stays machineâ€‘parsable.
5. **Prototype interactively**: Build the pipeline one command at a time, appending `| head` to preview intermediate results.

Mastering these small but powerful building blocks will save hours otherwise spent writing throwâ€‘away scripts, and it makes every step of your genome assembly, variant calling, or populationâ€‘genetic analysis **transparent, efficient, and reproducible**. Happy piping!
