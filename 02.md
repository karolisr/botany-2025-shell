## Package managers: Installing stuff

Research pipelines in population genetics and evolutionary biology juggle many CLI tools‚Äî**`samtools`, `bcftools`, `RAxML‚ÄëNG`, `IQ‚ÄëTREE`, `PLINK`, `FastQC`, `BLAST+`,** and more. Manually compiling each one wastes time and risks version drift that can invalidate downstream analyses. A package manager:

* **Pins exact versions** so workflows run identically on every machine
* **Resolves dependencies automatically** (e.g., HTSlib‚ÄØ‚Üî‚ÄØsamtools‚ÄØ‚Üî‚ÄØbcftools)
* **Delivers security patches** for system libraries without manual rebuilding
* **Provides one‚Äëline cleanup** to reclaim space after large FASTQ/VCF crunches

---

## Two ecosystems at a glance (no extra repositories required)

| Aspect                              | `apt` (Ubuntu/Debian)                                                              | `brew` (macOS‚ÄØ+‚ÄØLinux)                                                                                     |
| ----------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| Bio‚Äërelevant packages in core repos | `samtools`, `bcftools`, `mafft`, `raxml‚Äëng`, `iqtree`, `blast+`, `fastqc`, `plink` | Same in **Homebrew core** (`samtools`, `bcftools`, `raxml‚Äëng`, `iqtree`, `plink`, `fastqc`, `blast`, etc.) |
| Install root                        | System‚Äëwide `/usr/bin`, `/usr/lib` (needs `sudo`)                                  | User‚Äëlevel Cellar, symlinked to `/usr/local/bin` or `/opt/homebrew/bin` (no `sudo`)                        |
| Upgrade strategy                    | `sudo apt upgrade` / `sudo apt full-upgrade`                                       | `brew upgrade`                                                                                             |
| Cleanup & health check              | `sudo apt autoremove` / `sudo apt clean`                                           | `brew cleanup` & `brew doctor`                                                                             |

---

## Best‚Äëpractice workflow¬†(for standard repos only)

1. **Refresh metadata & upgrade**

   ```bash
   # Ubuntu
   sudo apt update && sudo apt upgrade -y

   # macOS / Linuxbrew
   brew update && brew upgrade
   ```

2. **Search & inspect before installing**

   ```bash
   apt search iqtree | head
   brew info iqtree
   ```

3. **Dry‚Äërun heavy installs** (good on shared HPC nodes)

   ```bash
   sudo apt install --simulate bcftools
   brew install --dry-run iqtree
   ```

4. **Install the toolchain**

   ```bash
   # Population genetics & phylogenomics
   sudo apt install samtools bcftools plink raxml-ng iqtree fastqc blast+

   # Same on macOS
   brew install samtools bcftools plink raxml-ng iqtree fastqc blast
   ```

5. **Pin versions for published pipelines**

   ```bash
   echo "iqtree hold" | sudo dpkg --set-selections    # Ubuntu
   brew pin iqtree                                    # macOS
   ```

6. **Audit & prune monthly**

   ```bash
   sudo apt autoremove --purge
   brew cleanup
   brew outdated
   ```

---

<!-- ## Examples

<details>
<summary>üß¨¬†Call SNPs from BAM to VCF on Ubuntu</summary>

```bash
sudo apt install samtools bcftools
samtools mpileup -Ou -f ref.fa sample.bam | \
  bcftools call -mv -Oz -o sample.vcf.gz
```

Matching **samtools‚ÄØ+‚ÄØbcftools** builds linked against the same HTSlib avoid ABI mismatches.

</details>

<details>
<summary>üå≥¬†Maximum‚Äëlikelihood phylogeny with IQ‚ÄëTREE on macOS</summary>

```bash
brew install iqtree
iqtree -s alignment.phy -m GTR+G -B 1000 -nt AUTO
open alignment.phy.treefile  # View in FigTree if installed via brew install --cask figtree
```

Homebrew compiles IQ‚ÄëTREE with native optimizations, accelerating bootstrap analyses.

</details>

<details>
<summary>üìä¬†Prepare genotype matrices for STRUCTURE / ADMIXTURE</summary>

```bash
sudo apt install plink
plink --vcf cohort.vcf --make-bed --out cohort
```

Using the package manager lets collaborators reproduce the exact `plink` build with a single command.

</details>

--- -->

## Troubleshooting quick‚Äëref

| Issue                                       | `apt` fix                                           | `brew` fix                                                                 |
| ------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |
| HTSlib version mismatch                     | `sudo apt install --only-upgrade htslib`            | `brew upgrade htslib samtools bcftools`                                    |
| Missing libraries when compiling R packages | `sudo apt install libcurl4-openssl-dev libxml2-dev` | `brew install libxml2 curl openssl@3 && brew link --force libxml2`         |
| Permission denied                           | Always prefix with `sudo` or check `/etc/sudoers`   | Ensure you own Homebrew dirs: `sudo chown -R $(whoami) $(brew --prefix)/*` |
| Conflicting files                           | `sudo apt --fix-broken install`                     | `brew link --overwrite <pkg>` or `brew unlink <pkg>`                       |

---

### Take‚Äëhome points for life‚Äëscience researchers

1. **Reproducibility first** ‚Äî lock tool versions before paper submission.
2. **Standard repos suffice** ‚Äî many bioinformatics staples (including **IQ‚ÄëTREE**) are in default `apt` and Homebrew core.
3. **Simulate big installs** on clusters to dodge quota surprises.
4. **Document your environment** (`apt list --installed > packages.txt`, `brew bundle dump`) and commit alongside your code.

With these habits, your phylogenetic trees, GWAS scans, and floristic datasets will stay **repeatable and transparent**‚Äîboosting confidence in every published clade and allele‚Äëfrequency plot. Happy analysing!

## Pipes

Below are five self‚Äëcontained mini‚Äëpipelines that chain **only standard GNU/Linux utilities** (`find`, `xargs`, `grep`, `awk`, `sort`, `uniq`, `cut`, `tr`, `wc`, `tee`,‚ÄØetc.). They‚Äôre meant as ‚Äúlego bricks‚Äù you can demo live to show how small commands combine into surprisingly rich workflows.

---

### 1.‚ÄØFind the ten most abundant IP addresses in a web‚Äëserver log

```bash
cat access.log |        # stream the whole log
awk '{print $1}' |      # 1st column is the client IP
sort |                  # bring identical IPs together
uniq -c |               # collapse & count duplicates
sort -nr |              # numeric reverse sort by hits
head -10                # show top 10
```

**Why it‚Äôs neat:** six short commands replace an entire log‚Äëanalysis script.

---

### 2.‚ÄØCount reads across **every** gzipped FASTQ in the project directory

```bash
find . -name '*.fastq.gz' -print0 |   # locate all FASTQ files
xargs -0 zcat |                       # concatenate & decompress in one pass
wc -l |                               # total number of lines
awk '{print $1/4 " reads"}'           # 4 lines per read ‚Üí read count
```

No temporary files, works on terabyte‚Äëscale data.

---

### 3.‚ÄØExtract coding sequences from a GFF3, then tally the ten longest genes

```bash
grep -P '\tCDS\t' annotation.gff3 |      # keep only CDS features
cut -f1,4,5,9 |                          # grab seq‚Äëid, start, end, attributes
awk -F'\t' '{len=$3-$2+1; print len"\t"$4}' |  # compute length, keep attributes
sort -nr | head -10                       # top 10 by length
```

Shows off tab‚Äëaware filtering plus on‚Äëthe‚Äëfly arithmetic in a single stream.

---

### 4.‚ÄØCompute the GC content of all contigs in a FASTA file

```bash
awk '/^>/ {if(len){gc/len*100; printf "\t%.2f%%\n", gc/len*100}; \
           len=0; gc=0; printf "%s", $0; next} \
     {len+=length($0); gc+=gsub(/[GCgc]/,"") } \
     END {gc/len*100; printf "\t%.2f%%\n", gc/len*100}' \
     genome.fa | column -t
```

A single `awk` script‚Äîpiped from nothing but the file‚Äîprints each header with its GC%.

---

### 5.‚ÄØLive progress bar while uncompressing, sorting, and deduplicating a giant VCF

```bash
pv big.vcf.gz |          # progress‚Äëview the bytes as they stream
zcat |                   # decompress
grep -v '^##' |          # drop meta‚Äëheader lines, keep data & column header
sort --buffer-size=1G |  # external merge sort without temp scripting
uniq |                   # remove duplicate variant lines
tee cleaned.vcf | wc -l  # save output *and* report final variant count
```

Demonstrates pipes branching (`tee`), in‚Äëpipe monitoring (`pv`), and memory‚Äësafe sorting.

---

### Key teaching points to highlight during the demo

* **Pipelines are just streams:** every tool reads‚ÄØstdin, writes‚ÄØstdout, so composition is trivial.
* **Stateless commands scale:** utilities don‚Äôt load entire files into RAM (unless told to), so they handle genomes or logs several‚ÄØGB large.
* **Swap tools freely:** replace `uniq -c` with `awk` math, or `grep` with `ripgrep`‚Äîthe interface stays the same.
* **Debug incrementally:** add `head`, `less`, or `tee` anywhere to peek at intermediate output without rerunning everything.

Each one‚Äëliner illustrates how small, well‚Äëfocused GNU programs become far more powerful when they ‚Äútalk‚Äù to each other through pipes. Encourage learners to treat these commands like puzzle pieces‚Äîand never be afraid to string together a few more!


### A (very) quick primer on *pipes*

On Unix‚Äëlike systems every program can read **standard input** (stdin) and write **standard output** (stdout).
The **pipe operator `|`** connects these streams, so the output of one tool becomes the input of the next‚Äîno temporary files, no extra disk I/O. Because most bioinformatics data (FASTA, FASTQ, GFF, VCF¬†‚Ä¶) is just text, you can assemble powerful ad‚Äëhoc workflows by chaining tiny, well‚Äëtested GNU utilities instead of writing bespoke scripts.

```
tool‚ÄëA   |   tool‚ÄëB   |   tool‚ÄëC
stdin ‚Üí stdout ‚Üí stdin ‚Üí stdout ‚Üí ‚Ä¶
```

Key benefits for scientists:

* **Reproducibility** ‚Äì every step is visible in one line; pop it into a README or Snakemake rule.
* **Scalability** ‚Äì tools stream data, so terabyte‚Äëscale files stay off your RAM.
* **Flexibility** ‚Äì swap any component (e.g., `grep` ‚Üí `rg`) without rewriting everything else.

---

## Five bioinformatics‚Äëflavoured pipelines

---

#### 1Ô∏è‚É£¬†How many raw reads are in *all* gzipped FASTQ files under the current project?

```bash
find . -name '*.fastq.gz' -print0 |   # locate every FASTQ
xargs -0 zcat            |            # stream‚Äëdecompress
wc -l                    |            # total line count
awk '{printf "%.0f reads\n", $1/4}'   # 4 lines per read
```

---

#### 2Ô∏è‚É£¬†Get the ten longest coding genes from a reference GFF3

```bash
grep -P '\tCDS\t' annotation.gff3 |    # keep only CDS features
awk -F'\t' '{len=$5-$4+1; print len"\t"$9}' |
sort -nr | head -10
```

Shows arithmetic in `awk`, numeric sorting, and quick ranking‚Äîno Python needed.

---

#### 3Ô∏è‚É£¬†Per‚Äëcontig GC‚Äëcontent table for a multi‚ÄëFASTA genome

```bash
awk '/^>/ {if(NR>1) printf("\t%.2f%%\n", gc/len*100);
           gc=len=0; printf "%s", $0; next}
     {len+=length; gc+=gsub(/[GCgc]/,"")}
     END {printf("\t%.2f%%\n", gc/len*100)}' genome.fa |
column -t
```

A single `awk` pass prints each header followed by its GC%.

---

#### 4Ô∏è‚É£¬†Filter a VCF to high‚Äëquality biallelic SNPs **and** count how many you keep

```bash
grep -v '^##' sample.vcf |          # drop meta‚Äëheader
awk '$7=="PASS" && length($4)==1 && length($5)==1' |  # SNP‚Äëonly
tee clean.vcf | wc -l
```

`tee` saves the filtered file *and* lets you pipe the same stream into `wc`.

---

#### 5Ô∏è‚É£¬†Real‚Äëtime progress while deduplicating a huge VCF dump

```bash
pv big.vcf.gz |          # progress bar (bytes / ETA)
zcat | grep -v '^##' | sort --buffer-size=1G | uniq | \
tee unique.vcf | awk 'END{print NR" unique variants"}'
```

Demonstrates monitoring (`pv`), memory‚Äësafe external sorting, branching with `tee`, and a final summary.

---

### Take‚Äëhome message

Think of each GNU utility as a **LEGO brick**: simple and reliable on its own, but unexpectedly powerful when snapped into a pipeline. Mastering `|` (plus redirection `>`, subshells `$( )`, and `tee`) will make every genome assembly, variant call‚Äëset, or read‚Äëcounting job faster to prototype‚Äîand easier for your collaborators to reproduce.

## A deeper look at ‚Äúpipes‚Äù for bioinformatics‚ÄØworkflows

When you type a command in the shell, it receives three default data streams:

| Stream     | File descriptor | Default destination         | Why bioinformaticians care                                          |
| ---------- | --------------- | --------------------------- | ------------------------------------------------------------------- |
| **stdin**  | `0`             | keyboard / previous command | Read huge FASTA/VCF files without loading them fully into RAM       |
| **stdout** | `1`             | terminal                    | Hand results to the *next* tool instead of creating temporary files |
| **stderr** | `2`             | terminal (error channel)    | Keep warnings separate from the data stream so pipelines stay clean |

The **pipe operator `|`** connects `stdout` of one program directly to `stdin` of the next:

```text
aligner ‚îÄ‚î¨‚îÄ‚ñ∫ stdout ‚îÇ
         ‚îÇ          ‚îÇ   sorter  ‚îÄ‚î¨‚îÄ‚ñ∫ stdout ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ          ‚îÇ   summarizer ‚îÄ‚îÄ‚ñ∫ results
                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why pipes shine in life‚Äëscience data crunching

* **Streaming, not loading** ‚Äì Most GNU tools process data line‚Äëby‚Äëline, so a 100‚ÄØGB BAM file never sits entirely in memory.
* **Composability** ‚Äì Each command does one thing well (Unix philosophy). Chain them to build ad‚Äëhoc ‚Äúmini‚Äëworkflows‚Äù in seconds.
* **Reproducibility** ‚Äì A single line in a README or Snakemake rule documents every step‚Äîno hidden GUI clicks.
* **Portability** ‚Äì Every modern Linux distribution ships the same coreutils, `grep`, `awk`, `sort`, `uniq`, etc.

---

## Five detailed, biology‚Äëcentric pipelines

Below each pipeline you‚Äôll find *what it does* and *why it matters*. All commands rely solely on tools that come with a fresh Ubuntu or Debian install (plus `pv`, which is in `apt` and Homebrew core).

---

### 1Ô∏è‚É£¬†Count raw reads in every gzipped FASTQ below the current directory

```bash
find . -name '*.fastq.gz' -print0 |     # ‚ë† list all FASTQ files, NUL‚Äëseparated
xargs -0 zcat            |              # ‚ë° decompress safely (NUL handles spaces)
wc -l                    |              # ‚ë¢ total number of lines
awk '{printf "%.0f reads\n", $1/4}'     # ‚ë£ convert lines ‚Üí reads (4 lines/read)
```

*Why it‚Äôs useful*
You can sanity‚Äëcheck a sequencing run before alignment and catch truncated uploads (read count mismatch) **without** ever writing decompressed data to disk.

*Key pipe concepts*
`find¬†‚Ä¶¬†-print0` + `xargs¬†-0` forms a bullet‚Äëproof pair that survives filenames with spaces or emojis.

---

### 2Ô∏è‚É£¬†Rank the longest coding sequences (genes) in a GFF3 annotation

```bash
grep -P '\tCDS\t' annotation.gff3 |              # ‚ë† keep only CDS rows
awk -F'\t' '{len=$5-$4+1; print len"\t"$1":"$4"-"$5"\t"$9}' | \
                                             # ‚ë° compute length; print length,
                                             #    coordinates, attributes
sort -nr | head -10                            # ‚ë¢ numeric reverse‚Äësort & top 10
```

*Why it‚Äôs useful*
Spotting unusually long proteins helps detect possible annotation errors or giant gene families worth further study.

*Key pipe concepts*
You can inline arithmetic (`awk`) and ranking (`sort | head`) without ever opening Python/R.

---

### 3Ô∏è‚É£¬†Produce a per‚Äëcontig GC‚Äëcontent table for a genome FASTA

```bash
awk '
/^>/ {                       # header line
  if (NR>1) printf "\t%.2f%%\n", gc/len*100;  # print previous contig stats
  gc=len=0;                               # reset counters
  printf "%s", $0; next                   # echo header without newline
}
{ len += length($0); gc += gsub(/[GCgc]/, "") }   # count bases & G/C
END { printf "\t%.2f%%\n", gc/len*100 }           # final contig
' genome.fa | column -t
```

*Why it‚Äôs useful*
GC bias affects read mapping, assembly, and variant calling. A quick per‚Äëcontig table can diagnose odd coverage patterns.

*Key pipe concepts*
A *single* `awk` script does aggregation, math, and formatted printing; `column` makes the output pretty without extra code.

---

### 4Ô∏è‚É£¬†Filter and count high‚Äëquality biallelic SNPs in a VCF

```bash
grep -v '^##' sample.vcf |                                 # ‚ë† strip meta‚Äëheaders
awk '$7=="PASS" && length($4)==1 && length($5)==1' |       # ‚ë° keep PASS + SNPs
tee clean_snps.vcf |                                       # ‚ë¢ save stream to file
wc -l |                                                    # ‚ë£ count remaining rows
awk '{print $1-1 " SNPs retained"}'                        # minus the #CHROM header
```

*Why it‚Äôs useful*
Demonstrates how you can **annotate** your own QC thresholds directly in the pipeline and verify the result in real time.

*Key pipe concepts*
`tee` branches the stream: one leg to disk, the other to `wc`, so no second pass is needed.

---

### 5Ô∏è‚É£¬†See live progress while deduplicating a massive VCF (or any large text file)

```bash
pv big.vcf.gz |                     # ‚ë† show MB/s, ETA
zcat |                              # ‚ë° decompress
grep -v '^##' |                     # ‚ë¢ drop meta‚Äëheaders
sort --buffer-size=1G |             # ‚ë£ external merge sort, 1‚ÄØGB RAM cap
uniq |                              # ‚ë§ collapse duplicates
tee unique.vcf |                    # ‚ë• save cleaned file
awk 'END{print NR-1" unique variants"}'  # ‚ë¶ final count (minus header)
```

*Why it‚Äôs useful*
Genotypers sometimes emit duplicate lines. This pipeline removes them **safely** on a laptop with limited memory while showing how long it will take.

*Key pipe concepts*

| Concept           | Command               | Why it helps                                  |
| ----------------- | --------------------- | --------------------------------------------- |
| **Monitoring**    | `pv`                  | Immediate feedback for multi‚Äëminute jobs      |
| **Resource caps** | `sort¬†--buffer-size=` | Prevents swapping on low‚ÄëRAM machines         |
| **Branching**     | `tee`                 | Simultaneous save + stats, no extra disk read |

---

### General best practices for piping in bioscience

1. **Fail early**: Insert `set¬†-o¬†pipefail` in Bash scripts so an error in any segment aborts the entire pipeline.
2. **Quote and NUL‚Äëterminate**: Use `find¬†-print0`/`xargs¬†-0` or `while¬†IFS= read -r -d '' file; do ‚Ä¶; done` loops to survive funky filenames.
3. **Document as you go**: Comment inline (`# ‚Ä¶`) so future you‚Äîor a peer reviewer‚Äîknows *why* each filter exists.
4. **Keep streams pure**: Reserve `stderr` (`>&2`) for warnings so `stdout` stays machine‚Äëparsable.
5. **Prototype interactively**: Build the pipeline one command at a time, appending `| head` to preview intermediate results.

Mastering these small but powerful building blocks will save hours otherwise spent writing throw‚Äëaway scripts, and it makes every step of your genome assembly, variant calling, or population‚Äëgenetic analysis **transparent, efficient, and reproducible**. Happy piping!
